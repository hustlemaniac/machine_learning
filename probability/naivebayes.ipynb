{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of gaussian naive bayes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0\n",
      " 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0\n",
      " 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0]\n",
      "Predicted Probabilities:  [[1.00000000e+00 5.52387327e-30]]\n",
      "Predicted Class:  [0]\n",
      "Truth: y=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "print(y)\n",
    "# define the model\n",
    "model = GaussianNB()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# select a single sample\n",
    "Xsample, ysample = [X[0]], y[0]\n",
    "# make a probabilistic prediction\n",
    "yhat_prob = model.predict_proba(Xsample)\n",
    "print('Predicted Probabilities: ', yhat_prob)\n",
    "# make a classification prediction\n",
    "yhat_class = model.predict(Xsample)\n",
    "print('Predicted Class: ', yhat_class)\n",
    "print('Truth: y=%d' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculate the probability of cancer patient and diagnostic test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A|B) = 0.339%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#without pyhton libraries\n",
    "# calculate P(A|B) given P(A), P(B|A), P(B|not A)\n",
    "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "\t# calculate P(not A)\n",
    "\tnot_a = 1 - p_a\n",
    "\t# calculate P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)\n",
    "    # P(Test=Positive) = P(Test=Positive|Cancer=True) * P(Cancer=True) + P(Test=Positive|Cancer=False) * P(Cancer=False)\n",
    "\tp_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
    "\t# calculate P(A|B)= P(B|A) * P(A) / P(B)\n",
    "    #P(Cancer=True | Test=Positive) = P(Test=Positive|Cancer=True) * P(Cancer=True) / P(Test=Positive)\n",
    "\tp_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "\treturn p_a_given_b\n",
    "\n",
    "# P(A)=base rate=people have cancer irrespective of the test\n",
    "p_a = 0.0002\n",
    "# P(B|A)=sensitivity=people with cancer will get a positive test result\n",
    "p_b_given_a = 0.85\n",
    "# P(B|not A)\n",
    "#P(not B/ not A)=specificity=testing negative result (Test=Negative) when the patient does not have cancer (Cancer=False)\n",
    "#P(Test=Negative | Cancer=False) = 0.95\n",
    "# P(Test=Positive|Cancer=False) = 1 – P(Test=Negative | Cancer=False)=1-0.95=0.05\n",
    "p_b_given_not_a = 0.05\n",
    "# calculate P(A|B)\n",
    "result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
    "# summarize\n",
    "print('P(A|B) = %.3f%%' % (result * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Naive Bayes Classification from sratch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X and y\n",
      "(100, 2) (100,)\n",
      "[[-0.79415228  2.10495117]\n",
      " [-9.15155186 -4.81286449]\n",
      " [-3.10367371  3.90202401]\n",
      " [-1.42946517  5.16850105]\n",
      " [-7.4693868  -4.20198333]]\n",
      "[0 1 0 0 1]\n",
      "Number of Xy0 and Xy1\n",
      "(50, 2) (50, 2)\n",
      "-1.5632888906409914 0.787444265443213\n",
      "4.426680361487157 0.958296071258367\n",
      "-9.681177100524485 0.8943078901048118\n",
      "-3.9713794295185845 0.9308177595208521\n",
      "P(y=0 | [-0.79415228  2.10495117]) = 0.348\n",
      "P(y=1 | [-0.79415228  2.10495117]) = 0.000\n",
      "Truth: y=0\n"
     ]
    }
   ],
   "source": [
    "# example of preparing and making a prediction with a naive bayes model\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import norm\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# fit a probability distribution to a univariate data sample\n",
    "# we are interested in the conditional probability of each input variable. This means we need one distribution for each of the input variables, and one set of distributions for each of the class labels, or four distributions in total\n",
    "def fit_distribution(data):\n",
    "\t# estimate parameters\n",
    "\tmu = mean(data)\n",
    "\tsigma = std(data)\n",
    "\tprint(mu, sigma)\n",
    "\t# fit distribution\n",
    "\tdist = norm(mu, sigma)\n",
    "\treturn dist\n",
    "\n",
    "# calculate the independent conditional probability\n",
    "def probability(X, prior, dist1, dist2):\n",
    "\t# P(yi | x1, x2, …, xn) = P(x1|yi) * P(x2|yi) * … P(xn|yi) * P(yi)\n",
    "    return prior * dist1.pdf(X[0]) * dist2.pdf(X[1])\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "print(\"Details of X and y\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[:5])\n",
    "print(y[:5])\n",
    "# The “random_state” argument is set to 1, ensuring that the same random sample of observations is generated each time the code is run.\n",
    "# The input and output elements of the first five examples are also printed, showing that indeed, the two input variables are numeric and the class labels are either 0 or 1\n",
    "# sort data into classes\n",
    "Xy0 = X[y == 0]\n",
    "Xy1 = X[y == 1]\n",
    "print(\"Number of Xy0 and Xy1\")\n",
    "print(Xy0.shape, Xy1.shape)\n",
    "#prior=P(yi)=#yi/#all events\n",
    "# 50% exactly given that we have created the same number of examples in each of the two classes;\n",
    "# calculate priors\n",
    "priory0 = len(Xy0) / len(X)\n",
    "priory1 = len(Xy1) / len(X)\n",
    "# create PDFs for y==0\n",
    "#Xy0[:,0]=column 0 in Xy0 \n",
    "#Xy0[:, 1]=column 1 in Xy0 \n",
    "#fit_distribution() function that we defined to prepare a probability distribution for each variable, for each class label\n",
    "distX1y0 = fit_distribution(Xy0[:, 0])\n",
    "distX2y0 = fit_distribution(Xy0[:, 1])\n",
    "# create PDFs for y==1\n",
    "distX1y1 = fit_distribution(Xy1[:, 0])\n",
    "distX2y1 = fit_distribution(Xy1[:, 1])\n",
    "# classify one example\n",
    "Xsample, ysample = X[0], y[0]\n",
    "py0 = probability(Xsample, priory0, distX1y0, distX2y0)\n",
    "py1 = probability(Xsample, priory1, distX1y1, distX2y1)\n",
    "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))\n",
    "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))\n",
    "print('Truth: y=%d' % ysample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee9dfcf03ef883f799e8db599acd57d78b13ea7c83670eb20ef8000875df171a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
